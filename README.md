# Address Book Manager - Back end

[For details about how front end is built, please check the [\[Front End Readme\]](./frontend/)]

Flask application with Open API (Swagger) standard compatible RESTful API.

### Update - Reflect and Additional thoughts - March 15th

* Several things that can be improved during another review of the code:
    * The Flask-RESTPLUS framework provides an adequate ways to parse response, but the modelling and validation on the input params are not idea. Colandar does it much better. Here we could either implement our own ways to handle API params, or use other libraries that are good at modelling and parsing params.
    * The upload file issue mentioned in the Google App Engine section. Not just files, for session context details, this would share the same problem, if such information is stored in the local machine.
      --> AWS provides duration based session affinity and application controlled session affinity on load balancers, both using cookies (either generated by the application or maintained by AWS itself).
      --> Google's solution is more towards saving session information into shared memcache or 3rd party key-value stores.
      For file uploading, AWS has S3 and Google App Engine has Cloud Storage
    * The framework of the CSV file lacks a strict business layer (the architecture has one layer in the resourceful API and one layer in the DB ORM, but lacks the layer that is tied to business logic). Considered that the currently only two columns are used in CSV and the only business logic here is to validate the email address, I opt-out the business layer, and instead uses a CSVHandler class to handle everything. But this approach is not scalable and will be problematic when there are many columns in the CSV files and requires extensive validation check against cells. A better approach is to do declarative design. A typical business model would look like:
```
class StringField(Field):
	type_ = str

class EmailField(StringField):
	max_length = MAXLENGTH_EMAIL_ADDRESS
	validator = EmailValidator

	def validate(self, row)
		self.validator(row.email).validate()

class Row(metaclass=sheet.RowMeta):
    name = sheet.StringField(max_length=64, required=True)
    email = sheet.EmailField(required=True)

	def validate(...):
		for fname, field in self.fields:
			field.validate(...)
```
The other thing can I can do better is to seperate out the dialect and properties of CSV handing (encoding, has_header, quotechar, etc) into a configuration class, say:
```
class CSVOptions:
	has_header = False
	quotechar = '"'
	delimiter = ' '
	
	def __init__(self, has_header, quotechar, delimiter):
	    ...
```
then when we are handling CSV, pass this object, instead of dictionaries into it. The objective in the end is to separate this part of the logic from the CSVHandler class, so all the checking is done here and CSVHandler simply responsible for reading CSV files.

### Demo

This application has been deployed to Google App Engine at:
`https://address-book-196923.appspot.com/`

### Installing

Create a new Python 3.5 virtualenv, and activate the env and run `pip install -r requirements.txt`.

For Google App Engine deployment, see the `app.yaml` file.

### Deployment

Set the Flask application global variable `FLASK_APP` to `app.py` and then simply execute `flask run` in the command line.

### Built With

* `Python 3`
* `Flask`
* `SQLAlchemy`
* `Alembic` for database migration
* `Flask-RESTPLUS` for Swagger compatible RESTful API
* `PostgreSQL` for database
* `nosetests` for unit tests (86% coverage)

### Endpoints
* `/ ` root for the main application
* `/api` RESTful API endpoint
* `/api_doc` Swagger RESTful API documentation

### Authors
Sheldon Rong

### A bit about Google App Engine

First of all, I have to say `gcloud app deploy` really takes a lot of time. (and it seems a lot of people are not happy :) )

Then there is a second issue I managed to solve, and it was caused by non-other than autoscaling.

Sometimes you love autoscaling, sometimes you don't :)

So the issue is: when the app is running in GAE, and when user upload the CSV file, there is about half chance you
get an error immediately after the file finished uploading, saying could not locate the file.

Initially I thought this is related to files sitting in /tmp or /var/tmp being aggressively deleted by the OS.
So had a bit tweeting of both code and the linux settings. Ha, I was so wrong.
It turns out Google auto scaled my instance, and when the file is uploaded, it is saved as a copy in one
of the instance container, not both. So that sort of explained why it is an intermittent issue.

Now obviously, this means there is a design flaw here, and to solve this, files has to be uploaded to a central
server, and in Google's case, the Google Cloud Storage, which would then need to use the SDK to connect.
However, due to limited time, I decided to go the other approach: remove one of the instance.

With this in mind, here are the details.

### License
This project is licensed under the MIT License
